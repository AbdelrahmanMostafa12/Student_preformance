# -*- coding: utf-8 -*-
"""project data mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ex9PKD3_Q2T758CBR-c-e2bXS_ZfY49t
"""

!pip install seaborn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

PATH = '/content/StudentsPerformance.csv'
raw_df = pd.read_csv(PATH)
df = raw_df
df.head()

print("Null Values Count")
df.isna().sum()

print("Duplicate Rows : ", df.duplicated().sum())

df.info()

df.nunique()

df.describe()

# Adding column of Average Score

df['average score'] = (df['math score']+df['reading score']+df['writing score']) / 3
df.head()

plt.figure(figsize=(7,4))

plt.title('Avg. Score Distribution', fontsize=15, fontweight='bold')
sns.histplot(data=df, x='average score',bins=30,kde=True,color='g')


plt.show()

gender_group = df.groupby('gender').mean()
gender_group

plt.figure(figsize=(10, 6))


X = ['Maths','Reading','Writing','Total Average']
Ygirls = gender_group.values[0]
Zboys = gender_group.values[1]
  
X_axis = np.arange(len(X))
  
plt.bar(X_axis - 0.2, Ygirls, 0.4, label = 'Female')
plt.bar(X_axis + 0.2, Zboys, 0.4, label = 'Male')
  
plt.xticks(X_axis, X)
plt.xlabel("<-- Scores -->")
plt.ylabel("Marks")
plt.title("Scores of both genders compared", fontweight='bold')
plt.legend()
plt.show()

#agg()allows you to apply a function or a list of function names to be executed along one of the axis of the DataFrame
#default 0, which is the index (row) axis.
df.groupby('parental level of education').agg('mean').plot(kind='barh',figsize=(8,6))
#bbox_to_anchor,loc and borderaxespad change location of the legend()
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
#y for the location of title
plt.title('Parental Level of Edu. & Scores', fontsize=11, fontweight=600,y=1.05,loc="left")

plt.show()

Group_data2=df.groupby('race/ethnicity')
#The subplot() function takes three arguments that describes the layout of the figure.
#if the number 3 changed to 2 we need to remove one of the barplot 
f,ax=plt.subplots(1,3,figsize=(20,7))
#palette RGB color strings
sns.barplot(x=Group_data2['math score'].mean().index,y=Group_data2['math score'].mean().values,palette = 'mako',ax=ax[0])
ax[0].set_title('Math score',color='#005ce6',size=17)

for container in ax[0].containers:
    ax[0].bar_label(container,color='black',size=12)

sns.barplot(x=Group_data2['reading score'].mean().index,y=Group_data2['reading score'].mean().values,palette = 'flare',ax=ax[1])
ax[1].set_title('Reading score',color='#005ce6',size=17)

for container in ax[1].containers:
    ax[1].bar_label(container,color='black',size=12)

sns.barplot(x=Group_data2['writing score'].mean().index,y=Group_data2['writing score'].mean().values,palette = 'viridis',ax=ax[2])
ax[2].set_title('Writing score',color='#005ce6',size=17)

for container in ax[2].containers:
    ax[2].bar_label(container,color='black',size=12)

sns.pairplot(data=df[['math score', 'reading score', 'writing score']], height = 4)

!pip install sklearn
!pip install svm
!pip install libsvm

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

def ScoreMark(score):
    if ( score > 90 ):
        mark = 'A'
    elif ( score > 80):
        mark = 'B'
    elif ( score > 70):
        mark = 'C'
    elif ( score > 60):
        mark = 'D'
    elif ( score > 50):
        mark = 'E'
    else: 
        mark = 'F'
    return mark
#The lambda function is used to apply this process to all values in the "math score", "reading score", and "writing score" columns, 
#respectively. The lambda function helps in quickly creating an anonymous function without the need to define a separate function each time
df['math mark'] = df['math score'].apply(lambda s: ScoreMark(s))
df['reading mark'] = df['reading score'].apply(lambda s: ScoreMark(s))
df['writing mark'] = df['writing score'].apply(lambda s: ScoreMark(s))

def getMarkData(dt, marks):
    subDt = dt[(dt['math mark'].isin(marks)) | (dt['reading mark'].isin(marks)) | (dt['writing mark'].isin(marks))]
    return subDt
    #shape[0] method to count the number of rows in each sub-DataFrame that contains a specific mark in the "math mark",
    # "reading mark", or "writing mark" columns. The counts for each subject are printed to the console.
def MarkCounts(dt, marks):
    subDt = getMarkData(dt, marks)
    print('Math: ' + str(subDt[subDt['math mark'].isin(marks)].shape[0])
      , '\n'
      , 'Writing: ' + str(subDt[subDt['writing mark'].isin(marks)].shape[0])
      , '\n'
      , 'Reading: ' + str(subDt[subDt['reading mark'].isin(marks)].shape[0])
      , '\n'
      , '\n'
      , 'Math and Reading: ' + str(subDt[(subDt['math mark'].isin(marks)) & (subDt['reading mark'].isin(marks))].shape[0])
      , '\n'
      , 'Math and Writing: ' + str(subDt[(subDt['math mark'].isin(marks)) & (subDt['writing mark'].isin(marks))].shape[0])
      , '\n'
      ,'Reading and Writing: ' + str(subDt[(subDt['reading mark'].isin(marks)) & (subDt['writing mark'].isin(marks))].shape[0])
      , '\n'
      , '\n',
      'All: '+str(subDt[(subDt['math mark'].isin(marks))&(subDt['reading mark'].isin(marks))&(subDt['writing mark'].isin(marks))].shape[0])
     )

print('F')
MarkCounts(df, ['F'])
print('\n A')
MarkCounts(df, ['A'])

figure = plt.figure(figsize=(16,4))
n = 1
for i in ['math score', 'reading score', 'writing score']:
    ax = figure.add_subplot(1, 3, n)
    ax.set_title(i)
    df[i].hist()
    n = n + 1

figure = plt.figure(figsize=(16,4))
n = 1
for i in ['math mark', 'reading mark', 'writing mark']:
    ax = figure.add_subplot(1, 3, n)
    ax.set_title(i)
    df[i].value_counts().sort_index().plot(kind="bar")
    n = n + 1

def hasFailed(dt):
    if ((dt['math mark'] == 'F') | (dt['reading mark'] == 'F') | (dt['writing mark'] == 'F')):
        return 1
    else:
        return 0
df['failed'] = df.apply(hasFailed, axis=1)

classification_data = df[[
                              'gender'
                            , 'race/ethnicity'
                            , 'parental level of education'
                            , 'lunch'
                            , 'test preparation course'
                            , 'failed'
                           ]]

text_columns = [
  'gender'
, 'race/ethnicity'
, 'parental level of education'
, 'lunch'
, 'test preparation course'] 
#pd.get_dummies() function. One-hot encoding is a process of converting categorical features into binary features, 
#where each category is transformed into a new column and assigned either 0 or 1 based on whether or not it is present in the original data.
classification_data = pd.get_dummies(classification_data, columns=text_columns)
classification_data.head(6)

#Splitting data into main training and validation datasets, 80/20
#The reset_index method is first used to reset the index of the classification_data dataframe
classification_data.reset_index(level=[0], inplace=True)
#the sample method is used to randomly sample 80% of the rows from classification_data to create data_train, 
#while the remaining 20% of the rows make up data_val. 
#The shape of the resulting data_train dataframe is printed, 
#showing the number of rows with a failed value of 0 and 1, as well as the shape of the data_val dataframe.
data_train = classification_data.sample(int(np.floor(classification_data.shape[0] * 0.8)), random_state=999)
data_val = classification_data[np.logical_not(classification_data['index'].isin(data_train['index']))]
data_train = data_train.drop(columns = ['index'])
data_val = data_val.drop(columns = ['index'])
print(data_train[data_train['failed'] == 0].shape
    , data_train[data_train['failed'] == 1].shape
     , data_val.shape)

#Oversampling: ge the needed oversample values

data_train_fail = data_train[data_train['failed'] == 1]
data_train_pass = data_train[data_train['failed'] == 0]

pass_n = data_train[data_train['failed'] == 0].shape[0]
fail_n = data_train[data_train['failed'] == 1].shape[0]
times_x = np.floor(pass_n / fail_n)
diff = int(pass_n - times_x * fail_n)

print(times_x, diff)

#Oversampling: concatenating oversampled data together.
data_train_over = pd.concat([data_train_pass, 
                            data_train_fail,
                            data_train_fail,
                            data_train_fail,
                            data_train_fail,
                            data_train_fail.sample(diff, random_state = 999)])
print(data_train_over[data_train_over['failed'] == 0].shape
    , data_train_over[data_train_over['failed'] == 1].shape
     , data_train_over.shape)

#test_train_sample
#Split arrays or matrices into random train and test subsets.Quick utility that wraps input validation,
X_train, X_test, y_train, y_test = train_test_split(
    data_train_over[data_train_over.columns.difference(['failed'])],
    data_train_over['failed'], test_size=0.3, random_state=999)

# Decision Tree
dt = DecisionTreeClassifier(criterion = "entropy", random_state = 999).fit(X_train, y_train)

acc = accuracy_score(y_test, dt.predict(X_test))


print(
     '\n decision tree', acc,
     
)

#difference()Return a new Index with elements of index not in other.This is the set difference of two Index objects
val_x = data_val[data_val.columns.difference(['failed'])]
val_y = data_val['failed']
train_x = data_train[data_train.columns.difference(['failed'])]
train_y = data_train['failed']


print('decision tree', accuracy_score(val_y, 
               DecisionTreeClassifier(criterion = "entropy", random_state = 999).fit(train_x, train_y).predict(val_x)))

from sklearn.metrics import confusion_matrix


print(' \n Decision Tree')
y_pred = DecisionTreeClassifier(criterion = "entropy", random_state = 999).fit(train_x, train_y).predict(val_x)
confusion_matrix_result = confusion_matrix(val_y, y_pred)
print("Confusion matrix:\n%s" % confusion_matrix_result)
tn, fp, fn, tp = confusion_matrix_result.ravel()
print('True Negative: ' + str(tn), ', False Positive: ' + str(fp), ', False Negative: ' + str(fn), ', True Positive: ' + str(tp))